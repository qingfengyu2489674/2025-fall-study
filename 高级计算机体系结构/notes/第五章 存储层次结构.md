## **5.1 存储系统引言与基本概念**

#### 一、 存储程序概念 (Stored-Program Concept)

-   **提出者**: 冯·诺依曼 (Von Neumann)。
-   **核心思想**: 这是现代计算机体系结构的基石。它规定了**程序和数据都以二进制的形式存放在存储器 (Memory)** 中，CPU 根据程序计数器 (PC) 的指示，**逐条 (One by one)** 从存储器中取出指令并执行。
-   **工作流程**:
    1.  将整个程序 (All) 加载到存储器中。
    2.  CPU 从存储器中逐条取出指令执行。

#### 二、 基本术语及概念：存储器类型

##### 1. 半导体存储器 (Semiconductor Memory)
-   **SRAM (Static RAM - 静态随机访问存储器)**:
    -   **特点**: 速度极快，只要供电，数据就能保持。
    -   **用途**: 通常用作高速缓存 (Cache)。
-   **DRAM (Dynamic RAM - 动态随机访问存储器)**:
    -   **特点**: 速度比SRAM慢，需要定时刷新来维持数据。成本较低，密度高。
    -   **用途**: 计算机的主存 (Main Memory)。
    -   **关键指标**: **容量** (如 2GB)、**存取周期** (如 60ns)。
-   **Flash Memory (闪存)**:
    -   **特点**: 非易失性（断电后数据不丢失），读写速度介于DRAM和磁盘之间。
    -   **用途**: U盘、固态硬盘 (SSD)、存储卡。

##### 2. 磁存储器 (Magnetic Memory)
-   **硬盘 (Hard Disk)**:
    -   **特点**: 容量大，成本低，非易失性，但速度远慢于半导体存储器。
    -   **关键指标**: **容量** (如 200GB)、**转速** (如 7200转/分)。
-   **磁带 (Magnetic Tape)**:
    -   **特点**: 容量巨大，成本极低，但访问方式为顺序访问，速度极慢。
    -   **现状**: 现在已经**过时 (Out)**，主要用于数据备份和归档。

##### 3. 光存储器 (Optical Memory)
-   **CD (Compact Disc)**:
    -   **CD-ROM**: 只读光盘。
    -   **WORM (Write-Once, Read-Many)**: 一次写入多次读取光盘。
    -   **CD-RW**: 可擦写光盘。
-   **DVD (Digital Versatile Disc)**: 容量比CD大得多。
-   **VCD (Video Compact Disc)**: 主要用于视频存储。

#### 三、 存储器的主要性能指标

1.  **存储容量 (Capacity)**:
    -   **定义**: 存储器能够存放的二进制信息总量。
    -   **单位**: 字节 (Byte)、KB (Kilobyte)、MB (Megabyte)、GB (Gigabyte) 等。

2.  **存取周期 (Access Cycle)**:
    -   **定义**: CPU **连续访存**中，**平均一次存取操作**所需的总时间。
    -   **说明**: 它不仅包括数据读取/写入的时间，还包括地址解码、总线传输等额外开销的时间。是衡量存储器速度的关键指标。

#### 四、 存储器存在的核心问题：“存储墙”

尽管存储程序概念是计算机的基础，但理想与现实之间存在巨大鸿沟。
-   **核心问题**: 理想中的存储器应该是无限大、无限快且价格低廉的，但现实中的存储器却存在两大问题：**太小、太慢**。
-   **处理器-存储器性能差距 (Processor-Memory Performance Gap)**:
    -   微处理器 (µProc) 的性能得益于**摩尔定律 (Moore's Law)**，以每年约 **60%** 的惊人速度增长。
    -   而主存 (DRAM) 的性能增长速度却非常缓慢，每年仅增长约 **7%**。
    -   这两者之间巨大的性能增速差异，导致了所谓的 **“CPU-DRAM Gap”** 或 **“存储墙 (Memory Wall)”**，这个差距以每年约 **50%** 的速度持续扩大。
-   **应对策略**:
    -   **1980年**: 微处理器还没有片上 Cache。
    -   **1989年**: Intel 首次集成 Cache 在芯片上。
    -   **1995年**: 芯片上已经集成了二级 Cache (L2 cache)。
    -   **解决方案**: 引入 **Cache (高速缓存)** 来弥补 CPU 与主存之间的性能差距。

#### 五、 不同类型计算机的存储层次结构关注点

不同的计算机系统，其存储层次结构的设计关注点也不同。
-   **桌面计算机 (Desktop Computer)**:
    -   最初为单用户运行一个应用程序设计。
    -   更关心来自存储器层次结构的**响应速度 (Latency)**。
-   **服务器 (Server)**:
    -   典型应用为上百个用户同时运行几十上百个应用程序。
    -   更关注**存储容量 (Capacity)** 和 **吞吐量 (Throughput)**。
-   **嵌入式计算机 (Embedded Computer)**:
    -   常用于实时应用。
    -   更关注**最坏情况性能 vs 最好情况性能**，以及**功耗和电池寿命**。
    -   通常主存很小，且**没有虚拟存储器**。

#### 六、 加快存储器速度的方法：存储器层次结构

##### 1. 基本原理与目标
-   **硬件的部件特征**:
    -   **更小**的硬件**更快**，但也**更贵**。
    -   **更大**的存储器级别**更低**，但也**更便宜**。
-   **设计目标**:
    -   构建一个存储系统，使其对外表现出：**具有最小存储器（如寄存器）的速度** 和 **最大存储器（如磁盘）的容量**。
    -   同时，其**价格接近最便宜级别的存储器**。

##### 2. 利用局部性原理 (Principle of Locality)
实现上述目标的关键在于利用程序的**局部性原理**：多数程序不会一下访问所有的代码或数据。
-   **时间局部性 (Temporal Locality)**: 如果一项被访问，则该项倾向于在**近期**又被访问。
    -   *利用方法*: 将最近访问过的多条数据项靠近处理器存放（如放在Cache中）。
-   **空间局部性 (Spatial Locality)**: 如果一项被访问，则其**附近**的项倾向于在**近期**被访问。
    -   *利用方法*: 将最近访问过的连续字（块）移向处理器存放。

##### 3. 方法：构建存储器层次结构 (Memory Hierarchy)
-   基于存储器的各**层 (级)** 具有不同的速度与大小。
-   **越靠近CPU的级**:
    -   **速度越快**
    -   **容量越小**
    -   **价格越贵** (每字节成本)

#### 七、 存储器层次结构金字塔模型

这是一个自顶向下、从快到慢、从小到大、从贵到便宜的层次结构：
-   **L0: 寄存器 (Registers)**
-   **L1: L1 缓存 (on-chip L1 cache, SRAM)**
-   **L2: L2 缓存 (off-chip L2 cache, SRAM)**
-   **L3: 主存 (main memory, DRAM)**
-   **L4: 本地二级存储 (local secondary storage, local disks)**
-   **L5: 远程二级存储 (remote secondary storage, distributed file systems, Web servers)**

-   **数据流动**: 数据以“块”为单位在相邻层次之间移动。L1从L2取，L2从主存取，主存从磁盘取。
-   **核心思想**: 利用局部性原理，**以最便宜技术提供尽可能多的存储空间**，同时**以最快的技术提供访问**。

#### 八、 Cache 是什么？

-   **定义**: **小而快的存储器**，用于改善对**慢速存储器**的**平均访问时间**。
-   **作用**: Cache 是存储器层次结构中的核心技术，它作为 CPU 和主存之间的缓冲，存放着主存中最常用数据的副本，使得 CPU 的大多数访存请求都能在高速的 Cache 中命中，从而避免了对慢速主存的直接访问。



## **5.2 Cache基本原理复习**

本节复习了 Cache 的核心概念、组织方式、工作原理以及设计中必须解决的四个关键问题。

#### 一、 Cache 术语 (Cache Terminology)
在深入学习前，需要了解一系列与 Cache 和存储层次结构相关的核心术语，例如：
-   **基本概念**: Cache, Block(块), Set(组), Valid bit(有效位), Dirty bit(脏位), Locality(局部性)
-   **性能指标**: Miss rate(失效率), Cache hit(命中), Miss penalty(失效开销), Average memory access time(平均访存时间)
-   **组织方式**: Direct mapped(直接映射), n-way set associative(n路组相联), Full associative(全相联)
-   **写策略**: Write through(写直达), Write back(写回), Write buffer(写缓冲), Write allocate(写分配)
- ...等等，总计36个核心术语。

#### 二、 Cache 组织的基本单位 —— 块 (Block / Line)

-   **通用概念 (Caching)**: “缓存”的思想是一个在**硬件、操作系统、文件系统和应用程序**中都广泛使用的一般概念，其本质是利用一个快速、小容量的存储来加速对慢速、大容量存储的访问。
-   **Cache的管理单位**:
    -   Cache 按**块 (Block)**（有时也称为**行 Line**）进行管理。
    -   Cache 和主存都被逻辑上分割成**大小相同的块**。
    -   CPU 对 Cache 和主存的数据交换，其**基本单位是块**。

#### 三、 Cache 的工作原理

-   **物理位置**: Cache 是一种高速存储器，**插入到 CPU 与主存之间**。现代计算机通常包含多级缓存，如 L1 Cache 和 L2 Cache。
-   **硬件实现**: 通常由速度极快的 **SRAM (静态随机访问存储器)** 实现。
-   **存储内容**: Cache 存储了主存中程序**部分**内容的**副本**，这些副本可以是**指令**，也可以是**数据**。

-   **基本操作**: 当 CPU 需要访问一个内存地址时：
    1.  CPU 首先在 Cache 中查找该地址对应的数据。
    2.  **命中 (Hit)**: 如果在 Cache 中找到了所需的数据块（如图中的红色块），则直接从 Cache 中高速读取，访问完成。
    3.  **未命中/失效 (Miss)**: 如果在 Cache 中**没有**找到所需的数据块（如图中的蓝色块），则：
        a. CPU 必须暂停（Stall）。
        b. 从慢速的主存中读取包含该数据的**整个块**。
        c. 将该块加载到 Cache 的某个位置。
        d. CPU 从 Cache 中重新读取该数据，恢复执行。

#### 四、 存储器层次结构设计者的 4 个关键问题

对于 Cache 的设计者而言，必须回答以下四个基本问题，这些问题的不同答案组合，构成了各种不同的 Cache 设计方案：

##### **问题1：映射规则 (Placement Policy) - 块可以放到哪里？**
-   **问题描述**: 当把一个块从主存调入 Cache 时，可以把它放置到 Cache 的哪些位置上？
-   **主要方案**:
    1.  **直接映射 (Direct Mapped)**: 主存中的每一个块只能映射到 Cache 中**唯一一个**固定的位置。
    2.  **全相联 (Fully Associative)**: 主存中的任何一个块可以被放置到 Cache 中**任意一个**位置。
    3.  **组相联 (Set Associative)**: 主存中的每一个块只能映射到 Cache 中**某一个特定组 (Set)** 里的**任意一个**位置。（这是前两种方案的折中）

##### **问题2：块识别 (Identification Policy) - 如何判断块是否命中？**
-   **问题描述**: 如何判断所需访问的块当前就在 Cache 中？
-   **主要方案**:
    -   使用**标记 (Tag)**。Cache 中的每一块都附带一个标记，该标记存储了主存地址的一部分信息。通过比较CPU发出的地址中的标记部分和 Cache 中对应块的标记，来判断是否命中。

##### **问题3：替换策略 (Replacement Policy) - 发生失效时替换哪一块？**
-   **问题描述**: 当访问的块不在 Cache 中，且 Cache 中没有空闲位置时（即发生**冲突失效**或**容量失效**），应该替换掉哪一个旧的块？
-   **主要方案**:
    -   **随机 (Random)**: 随机选择一块进行替换。
    -   **LRU (Least-Recently Used - 最近最少使用)**: 替换最长时间没有被访问过的块。
    -   **FIFO (First-In, First-Out - 先进先出)**: 替换最早进入 Cache 的块。

##### **问题4：写策略 (Write Policy) - 对块进行写时，该怎么办？**
-   **问题描述**: 当 CPU 对 Cache 中的数据块进行写操作时，如何保证 Cache 与主存中数据的一致性？
-   **主要方案**:
    1.  **写回 (Write Back)**: 写操作只在 Cache 中进行，同时将该块标记为“脏块 (Dirty)”。只有当这个脏块被替换出 Cache 时，才把它写回到主存。
    2.  **写直达 (Write Through)**: 写操作同时在 **Cache** 和**主存**中进行。为了避免CPU等待慢速的主存写操作，通常会配合一个**写缓冲 (Write Buffer)**。



好的，这是对“问题1：映像规则”和“问题2：块识别”这两部分PPT内容的详细、完整的总结。

---

### **问题1：映像规则 (Placement Policy)**

映像规则决定了一个主存块可以被放置到 Cache 中的哪些位置。

#### 一、 三种基本的映像规则

1.  **直接映射 (Direct Mapped)**
    -   **规则**: 主存中的一个块只能放在 Cache 中**唯一的一个**指定位置。
    -   **地址计算**: `Cache块地址 = (主存块地址) MOD (Cache中的总块数)`
    -   **优点**: 硬件实现最简单，查找速度快。
    -   **缺点**: 冲突率高。如果两个频繁访问的主存块映射到同一个Cache位置，会导致不断的替换和失效。

2.  **全相联 (Fully Associative)**
    -   **规则**: 主存中的一个块可以放在 Cache 中的**任意位置**。
    -   **地址计算**: 无映射计算，需要并行比较所有块。
    -   **优点**: 冲突率最低，Cache空间利用率最高。
    -   **缺点**: 硬件实现最复杂、最昂贵，查找速度慢（需要并行比较器）。

3.  **组相联 (Set Associative)**
    -   **规则**: 主存中的一个块能够放在 Cache 中**特定一组 (a set) 内的任意一个**块位置。
    -   **地址计算**: `Cache组号 = (主存块地址) MOD (Cache中的总组数)`
    -   **定义**: 一个**组 (Set)** 是 Cache 中一组块的集合。如果一组有 **n** 块，则该 Cache 称为 **n 路组相联 (n-way set associative)**。
    -   **特点**: 这是直接映射和全相联的折中方案，在冲突率和硬件成本之间取得了良好的平衡，是现代处理器中最常用的方式。

#### 二、 三种规则的统一视角

-   **直接映射**可以看作是 **1 路组相联 (1-way set associative)**，即每组只有一块。
-   **全相联**可以看作是 **m 路组相联 (m-way set-associative)**，其中 m 是 Cache 的总块数，即整个 Cache 只有一组。

---

### **问题2：块识别 (Identification Policy)**

块识别机制用于判断 CPU 请求的内存地址是否在 Cache 中，即是否**命中 (Hit)**。

#### 一、 地址的划分

为了实现块的识别和数据的定位，一个主存地址 (Block Address) 通常被划分为三个字段：

| 字段         | 标记 (Tag)                              | 索引 (Index)                          | 块内偏移 (Offset)                     |
| :----------- | :-------------------------------------- | :------------------------------------ | :------------------------------------ |
| **作用**     | 用于与Cache中的标记进行比较，**识别块** | 用于在Cache中**选择组 (Selects set)** | 用于在块内**选择数据 (Selects data)** |
| **存储位置** | 存储在Cache中                           | (不存储，用于寻址)                    | (不存储，用于寻址)                    |

-   **块内偏移 (Byte Offset)**:
    -   **作用**: 指明所需数据在块内的具体字节位置。
    -   **位数**: `log2(块大小，以字节为单位)`

-   **索引 (Index)**:
    -   **作用**: 指明主存块应该映射到 Cache 的哪一个**组**。
    -   **位数**: `log2(Cache中的总组数)`

-   **标记 (Tag)**:
    -   **作用**: 唯一标识一个主存块。当通过索引找到对应的组后，CPU会将地址中的Tag字段与该组内所有块的Tag字段进行并行比较。
    -   **位数**: `(物理地址总位数) - (索引位数) - (块内偏移位数)`

#### 二、 识别过程

1.  CPU 发出一个内存地址。
2.  硬件使用地址中的 **Index** 字段来定位到 Cache 中的特定一组。
3.  硬件并行地将地址中的 **Tag** 字段与该组内所有块的 **Tag** 字段进行比较。
4.  同时，检查对应块的**有效位 (Valid bit)** 是否为 `1` (表示该块数据有效)。
5.  如果**标记匹配**且**有效位为1**，则 **Cache 命中 (Cache Hit)**。
6.  命中后，使用地址中的 **Offset** 字段从数据块中提取出所需的字节。
7.  如果标记不匹配或有效位为0，则 **Cache 失效 (Cache Miss)**。

#### 三、 示例分析

##### 1. 直接映射 Cache (1-way)
-   **地址划分**: `Tag | Index | Offset`
-   **查找**: 使用 `Index` 直接定位到 Cache 的唯一一个块。比较该块的 `Tag` 和地址的 `Tag`。

##### 2. 全相联 Cache (m-way)
-   **地址划分**: `Tag | Offset`
-   **查找**: **没有 Index 字段** (因为只有一组)。需要将地址的 `Tag` 与 Cache 中**所有块**的 `Tag` **并行比较**。

##### 3. 2路组相联 Cache (2-way)
-   **地址划分**: `Tag | Index | Offset`
-   **查找**: 使用 `Index` 定位到包含**两块**的一组。将地址的 `Tag` 与这两块的 `Tag` **并行比较**。

#### 四、 思考题计算
-   **已知**: Cache容量=16KB, 块大小=32B, 2路组相联, 物理地址=36位。
-   **计算**:
    -   **块内偏移**: `块大小 = 32B = 2^5`  =>  **5位**
    -   **组数**: `(16 * 1024) / (2 * 32) = 16384 / 64 = 256` 组
    -   **索引**: `组数 = 256 = 2^8`  =>  **8位**
    -   **标记**: `36 - 8 - 5`  =>  **23位**



### **问题3：块替换 (Replacement Policy)**

当发生 Cache 失效，并且需要加载的块所对应的组 (Set) 中已经没有空闲位置时，就必须选择组内的一个现有块进行替换。

#### 一、 替换需求分析
-   **直接映射**: 只有一个候选位置，无需选择，直接替换。
-   **组相联 / 全相联**: 组内有 **N** 个块可供选择，需要一种策略来决定替换哪一个。

#### 二、 三种主要的替换策略
1.  **随机替换 (Random)**
    -   **策略**: 随机选择被替换的一块。
    -   **优点**: 硬件容易实现（需要一个伪随机数产生器），能够均匀使用组内的各块。
    -   **缺点**: 可能会“运气不好”，替换掉马上就要被访问的那一块。

2.  **最近最少使用 (LRU - Least-Recently Used)**
    -   **策略**: 选择一组中**最近最少被访问**的块作为被替换块。
    -   **优点**: 基于局部性原理（最近被访问的块很可能会再次访问），理论上命中率较高。
    -   **缺点**: 硬件实现复杂，需要在 Cache 中用额外的位来记录访问历史。

3.  **先进先出 (FIFO - First-In, First-Out)**
    -   **策略**: 选择一组中**最先进入 Cache** 的一块进行替换。
    -   **优点**: 硬件实现比 LRU 简单。
    -   **缺点**: 可能会替换掉一个很早就进入但至今仍在被频繁访问的块。

#### 三、 替换策略示例分析
通过一个具体的块地址访问序列 `1, 2, 4, 1, 3, 7, 0, 1, 2, 5`，在一个2路组相联（共2组4块）的 Cache 中，可以看到不同策略的效果。
-   **LRU 和 FIFO**: 在这个特定的访问序列下，最终的命中率都是 **1/10**。
-   **序列变化**: 如果访问序列稍作改变，例如 `1, 2, 4, 3, 1, ...`，LRU 和 FIFO 的行为和命中率就会产生差异，这凸显了替换策略对性能的直接影响。



### **问题4：写策略 (Write Policy)**

写策略定义了当 CPU 执行写操作时，如何处理 Cache 和主存之间的数据一致性问题。

#### 一、 写命中 (Write Hit) 时的策略

当要写的块**在 Cache 中**时，有两种主要策略：

1.  **写直达 (Write-Through)**
    -   **策略**: 数据**同时写入 Cache 和主存**。
    -   **优点**:
        -   **实现简单**，主存（或其他处理器）总是有最新的数据，数据一致性好。
        -   Cache 控制位只需要一个 **`valid bit`**。
    -   **缺点**:
        -   **速度慢**，CPU 必须等待慢速的主存写操作完成，产生**写停顿 (Write Stall)**。
        -   **带宽占用高**，每次写操作都会占用存储器总线。
    -   **优化**: 使用**写缓冲 (Write Buffer)**。CPU 将数据快速写入写缓冲后即可继续执行，由写缓冲在后台负责将数据慢慢写入主存。但这不能完全消除停顿，当写操作过于频繁导致缓冲溢出时，停顿依然会发生。

2.  **写回 (Write-Back)**
    -   **策略**: 数据**只写入 Cache**，不立即写入主存。同时，将该 Cache 块标记为**“脏 (dirty)”**。
    -   **数据同步**: 只有当这个“脏块”被替换出 Cache 时，才会被写回到主存。
    -   **优点**:
        -   **写 Cache 的速度更快**，因为不涉及慢速主存。
        -   **主存带宽更低**，因为多次对同一块的写操作最终只会导致一次主存写入。
    -   **缺点**:
        -   **实现复杂**，主存中的数据可能是过时的。
        -   Cache 控制位需要 **`valid bit`** 和 **`dirty bit`** 两个。

#### 二、 写失效 (Write Miss) 时的策略

当要写的块**不在 Cache 中**时，同样有两种策略选择：

1.  **写分配 (Write Allocate)**
    -   **策略**: 先把所写单元所在的**块从主存调入 Cache**，然后再进行**写命中**操作。
    -   **特点**: 行为与**读失效 (Read Miss)** 类似。

2.  **不按写分配 / 写绕过 (Write Around / No-Write Allocate)**
    -   **策略**: **直接将值写入下一级存储器**（如主存），而**不将**相应的块调入 Cache。
    -   **特点**: 写的值**不在** Cache 中。

-   **常规组合**:
    -   **写回 (Write-Back)** Cache 通常采用**写分配 (Write Allocate)**。
    -   **写直达 (Write-Through)** Cache 通常采用**不按写分配 (No-Write Allocate)**。

-   **示例分析**:
    -   **访问序列**: `write M[100]`, `write M[100]`, `read M[200]`, `write M[200]`, `write M[100]`
    -   **不按写分配 (No-write allocate)**:
        -   Misses: 1 (写M[100]直接写主存), 2 (写M[100]命中, 但之前没调入), 3 (读M[200]调入), 5 (写M[100]命中, 但之前没调入) -> **实际应为 1, 3**
        -   Hits: 2, 4, 5 -> **实际应为 2, 4, 5**
        *(注：此处PPT的答案`misses:1,2,3,5`可能存在歧义或错误，通常写绕过时后续同地址的写操作会被合并到写缓冲中，不计为新的miss。而`read M[200]`是一次明确的miss。更严谨的分析见下文)*
    -   **写分配 (Write allocate)**:
        -   Misses: 1 (写M[100]调入), 3 (读M[200]调入)
        -   Hits: 2 (写M[100]命中), 4 (写M[200]命中), 5 (写M[100]命中)

*(对示例的精确解读：*
*   **写分配**: 1. miss(调入100), 2. hit, 3. miss(调入200), 4. hit, 5. hit.  **总计 2 misses, 3 hits**
*   **不按写分配**: 1. miss(只写主存), 2. miss(只写主存), 3. miss(调入200), 4. hit, 5. miss(只写主存). **总计 4 misses, 1 hit**
*   *PPT答案可能基于不同假设，但基本策略逻辑不变*)*

#### 三、 实例：Alpha 21264 数据 Cache

这是一个真实的 Cache 设计案例，展示了各种概念的实际应用。
-   **规格**: 64KB, 块大小64字节 (1K个块), 2路组相联 (512个组)。
-   **Load 操作流程**:
    1.  **第1步**: CPU 提供地址，硬件将其分解为 Tag(<29>), Index(<9>), Offset(<6>)。
    2.  **第2步**: 使用 **Index** 字段选择所在的组，并同时读取该组内**两个**块的 Tag 位和数据。
    3.  **第3步**: **并行比较** CPU 地址的 Tag 和从 Cache 中读出的两个 Tag。
    4.  **第4步**:
        -   如果有一个 **Tag 匹配**（且 Valid 位为1），则为**命中**。使用 **Mux (多路选择器)** 从两个数据通路中选择正确的数据送给 CPU。
        -   如果没有 Tag 匹配，则为**失效**，从下一级存储器读取数据。
-   **性能**: Alpha 21264 允许在 **3个时钟周期**内完成这4步操作。
-   **写数据流程**:
    -   **读缺失 (Read Miss)**: 发出信号，从下一级读取，并使用 **LRU** 策略替换块。
    -   **写回 (Write-Back)**: 使用 **1 位 dirty bit** 记录块是否被修改。替换脏块时，需将数据送至**牺牲缓冲 (Victim Buffer)** / 写缓冲，再由其写入下一级存储。
    -   **写分配 (Write Allocate)**: 为读缺失和写缺失都分配 Cache 块。

#### 四、 指令 Cache 与数据 Cache：统一 vs 分离

-   **问题**: 处理器既要取指令，也要读写数据，这些操作应该使用同一个 Cache 还是分开的 Cache？
-   **统一 Cache (Unified Cache)**:
    -   **优点**: 动态平衡指令和数据的缓存空间，利用率可能更高。
    -   **缺点**: 可能会产生**结构冲突**。当流水线处理器执行 `LOAD` 或 `STORE` 指令时，可能需要同时请求一个数据字和一个指令字，如果都访问同一个 Cache，就会导致冲突和 CPU 等待。
-   **分离 Cache (Split Cache)**:
    -   **设计**: 一个专门存放指令的 **指令 Cache (I-Cache)**，另一个专门存放数据的 **数据 Cache (D-Cache)**。
    -   **优点**: 解决了结构冲突问题，可以同时满足指令和数据的访问请求。
    -   **现状**: **大多数最近生产的处理器都采用了分离的 Cache** (至少在L1级别)。Alpha AXP 21064 就是如此。
-   **性能对比**:
    -   **指令 Cache** 的失效率通常远低于**数据 Cache**，因为程序的代码局部性通常比数据局部性更好。
    -   **示例**:
        -   16KB分离Cache的总失效率: `(75% × 0.64%) + (25% × 6.47%) = 2.10%`
        -   32KB混合Cache的总失效率: `1.99%`
    -   **结论**: 在这个例子中，虽然 32KB 混合 Cache 的**失效率略低**，但这**并不意味着**混合 Cache 就一定更好，因为它没有考虑分离 Cache 在避免**结构冲突**方面带来的巨大性能优势。、



## **5.3 Cache 性能分析**

本节深入探讨了如何量化分析 Cache 对计算机整体性能的影响，并介绍了衡量 Cache 性能的关键指标和公式。

#### 一、 关键性能指标：平均访存时间 (AMAT)

-   **问题**: **失效率 (Miss Rate)** 虽然是一个常用指标，因为它与硬件速度无关，但它**不够完善**。
-   **更好的指标**: **平均访存时间 (Average Memory Access Time, AMAT)**，它综合考虑了命中和失效两种情况。
-   **AMAT 公式**:
    `AMAT = 命中时间 + 失效率 × 失效开销`
    -   **命中时间 (Hit Time)**: 访问 Cache 命中时所用的时间。
    -   **失效开销 (Miss Penalty)**: 发生 Cache 失效后，从下一级存储器获取数据并送回 CPU 所需的**额外**时间。
-   **分离 Cache 的 AMAT**:
    当采用分离的指令和数据 Cache 时，总的 AMAT 需要按访问类型加权平均：
    `总AMAT = (指令访存占比 × AMAT_指令) + (数据访存占比 × AMAT_数据)`
-   **局限性**: AMAT 仍然是一个**间接**指标。它比失效率更好，但**并不能完全代表 CPU 的执行时间**。

#### 二、 将 Cache 性能与 CPU 执行时间关联

-   **最终目标**: 减少 **CPU 的执行时间**。
-   **CPU 执行时间公式**:
    `CPU 执行时间 = (CPU 时钟周期数 + 存储器停顿周期数) × 时钟周期时间`
-   **存储器停顿周期数**:
    `= 指令数 × (每条指令的存储器访问次数) × 失效率 × 失效开销`
    `= (存储器访问总次数) × 失效率 × 失效开销`
-   **将 Cache 因素整合进 CPU 性能公式**:
    `CPU 时间 = 指令数 × (CPI_执行 + (每条指令访存次数 × 失效率 × 失效开销)) × 时钟周期时间`
    -   **`CPI_执行`**: **理想 CPI**，即**没有考虑任何存储器停顿**时的 CPI，只包括 ALU 指令和寄存器指令。
-   **Cache 的巨大影响**:
    -   一个简单的例子表明，若不采用 Cache，每次访存都需要承受巨大的失效开销，会导致**总 CPI 急剧增加**（如从3.33增加到68.5，超过30倍！），性能严重下降。

#### 三、 性能分析实例

##### 1. 实例一：分离 Cache vs. 混合 Cache
-   **背景**:
    -   16KB 分离 Cache (I-Cache + D-Cache) vs. 32KB 混合 Cache。
    -   命中时间 = 1个周期，失效开销 = 50个周期。
    -   混合 Cache 因为有结构冲突，`LOAD/STORE` 操作的命中时间增加1个周期。
-   **计算**:
    -   **分离 Cache AMAT**:
        `= 75% × (1 + 0.64% × 50) + 25% × (1 + 6.47% × 50) = 2.05` 周期
    -   **混合 Cache AMAT**:
        `= 75% × (1 + 1.99% × 50) + 25% × (1 + 1 + 1.99% × 50) = 2.24` 周期
-   **结论**: 尽管分离 Cache 的**实际失效率更高** (2.10% vs 1.99%)，但由于它避免了结构冲突（即命中时间更短），其**平均访存时间反而更低**。

##### 2. 实例二：直接映射 Cache vs. 2路组相联 Cache
-   **背景**:
    -   理想 CPI = 2.0，时钟周期 = 2ns，每条指令访存1.3次。
    -   两种 Cache 容量均为64KB，块大小32B，命中时间1个周期。
    -   **关键权衡**: 2路组相联 Cache 因为需要多路选择器，导致 CPU 的**时钟周期增加到原来的1.10倍** (2.2ns)。
    -   直接映射失效率 = 1.4%，2路组相联失效率 = 1.0%。
    -   失效开销 = 70ns。
-   **计算 AMAT**:
    -   `AMAT_1路 = 2.0ns + 0.014 × 70ns = 2.98 ns`
    -   `AMAT_2路 = 2.2ns + 0.010 × 70ns = 2.90 ns`
    -   **结论1**: 2路组相联 Cache 的**平均访存时间更低**。
-   **计算 CPU 时间**:
    -   `CPU 时间_1路 = IC × (2.0 × 2.0ns + 1.3 × 0.014 × 70ns) = IC × 5.27 ns`
    -   `CPU 时间_2路 = IC × (2.0 × 2.2ns + 1.3 × 0.010 × 70ns) = IC × 5.31 ns`
    -   `(CPU 时间_2路) / (CPU 时间_1路) = 5.31 / 5.27 ≈ 1.01`
-   **最终结论**:
    -   尽管2路组相联 Cache 降低了失效率和 AMAT，但它对**时钟周期的负面影响**（增加了10%）超过了其带来的好处。
    -   最终，**直接映射 Cache 的 CPU 执行时间反而更短**。
    -   **评价的最终基准是 CPU 时间**，直接映射 Cache 是本例中更好的选择。

#### 四、 怎样改善 Cache 性能？

所有改善 Cache 性能的优化措施，最终都可以归结为对**平均访存时间 (AMAT)** 公式的三个基本组成部分进行优化：

`平均访存时间 (AMAT) = 命中时间 (Hit Time) + 失效率 (Miss Rate) × 失效开销 (Miss Penalty)`

以下是针对这三个关键因素的12类优化措施：

---

##### 一、 降低失效率 (Miss Rate) 的措施

这类方法旨在让更多的数据访问能够在 Cache 中命中。

1.  **增大块容量 (Larger Block Size)**: 利用空间局部性，一次性载入更多相邻数据，可能会命中后续访问。
2.  **增大 Cache 容量 (Larger Cache Size)**: 减少容量失效，可以直接容纳更多的数据。
3.  **更高相联度 (Higher Associativity)**: 减少冲突失效，为主存块提供更多可选的存放位置。
4.  **编译器优化 (Compiler Optimizations)**: 通过重排代码或数据（如循环交换、分块），改善程序的局部性，使其访存模式更“Cache友好”。

---

##### 二、 减少失效开销 (Miss Penalty) 的措施

这类方法旨在缩短从下一级存储器获取数据所需的时间。

5.  **两级（多级）缓存 (Multi-level Caches)**: 在 L1 Cache 和主存之间增加一个速度和容量都居中的 L2 Cache。L1 失效时，从更快的 L2 获取数据，其失效开销远小于直接访问主存。
6.  **关键字优先和提前重启 (Critical Word First & Early Restart)**: 在一个数据块从主存传输到 Cache 的过程中，一旦 CPU 需要的那个“关键字”最先到达，就立即将其传送给 CPU，让 CPU 提前“重启”，而无需等待整个块传输完毕。
7.  **读缺失优于写缺失 (Giving Read Misses Priority over Writes)**: 由于读操作通常会直接导致 CPU 停顿，而写操作可以通过写缓冲来隐藏延迟，因此当读缺失和写操作同时发生时，优先处理读缺失。
8.  **牺牲缓冲 (Victim Caches)**: 在 Cache 和下一级存储之间增加一个小的、全相联的缓冲，用于存放被替换出去的块。如果 CPU 很快又需要这个被替换的块，就可以从牺牲缓冲中快速取回。
9.  **非阻塞缓存 (Non-blocking Caches)**: 允许 Cache 在处理一次失效的过程中，继续响应其他的命中请求，从而实现“失效下命中 (Hit Under Miss)”，减少停顿。
10. **硬件预取 (Hardware Prefetching)**: 硬件通过监测访存模式，主动将预测的、即将被访问的数据块从下一级存储提前加载到 Cache 中。

---

##### 三、 缩短命中时间 (Hit Time) 的措施

这类方法旨在加快对 Cache 本身的访问速度。

11. **小而简单的 Cache (Small and Simple Caches)**: 通常指 L1 Cache。更小的容量、更低的相联度和更简单的设计可以减少硬件延迟，从而缩短时钟周期或在单个周期内完成访问。
12. **写操作流水线 (Pipelined Writes)**: 将写操作分解为多个流水线阶段，提高吞吐率。



## **5.4 降低Cache失效率的方法**

为了有效地降低 Cache 失效率，首先需要理解导致失效的不同原因。

#### 一、 Cache 失效的三种分类 (“3C”模型)

按照产生失效的原因不同，可以把 Cache 失效分为以下三类：

1.  **强制性失效 (Compulsory Miss)**
    -   **原因**: 当**第一次**访问一个块时，该块肯定不在 Cache 中，需从下一级存储器中调入 Cache。
    -   **别名**: 也称为**首次访问失效 (Cold Start Miss)**。
    -   **特点**: 这种失效是**不可避免**的。无论 Cache 多大、相联度多高，只要是第一次访问，必然会发生。

2.  **容量失效 (Capacity Miss)**
    -   **原因**: 如果 Cache 的容量**容纳不了一个程序执行所需要的所有块**，就会发生容量失效。
    -   **过程**: 某些块因为 Cache 容量不足而被丢弃，随后又被重新调入。
    -   **特点**: 这种失效的发生与 Cache 的**大小**直接相关。

3.  **冲突失效 (Conflict Miss)**
    -   **原因**: 在**组相联**或**直接映射** Cache 中，若太多的块映射到同一组（或同一个块位置）中，则某一个块将被放弃，之后再重新调入时，发生了冲突失效。
    -   **特点**: 即使 Cache 的总容量足够大，也可能因为映射规则的限制而发生。这种失效与 Cache 的**相联度**直接相关。

#### 二、 “3C”模型的数据分析与洞察

通过对 SPEC92 典型程序在不同 Cache 配置下的失效数据进行分析，可以得出以下重要结论：

##### 1. 相联度的影响
-   **结论**: **相联度越高，冲突失效就越少**。
    -   从数据表和堆叠图中可以清晰地看到，随着相联度从1路（直接映射）增加到8路，冲突失效所占的比例和绝对值都显著下降。
-   **其他影响**: **强制性失效和容量失效不受相联度的影响**。相联度只改变块的存放位置，不改变 Cache 的总容量和程序的首次访问行为。

##### 2. Cache 容量的影响
-   **结论**: **强制性失效不受 Cache 容量的影响，但容量失效却随着容量的增加而减少**。
    -   强制性失效取决于程序首次访问多少个不同的块，与 Cache 大小无关。
    -   更大的 Cache 可以容纳更多的工作集，因此容量失效会随着 Cache 容量的增大而降低。

##### 3. Cache 经验规则
-   **2:1 的 Cache 经验规则**: 大小为 **N** 的**直接映射 Cache (1路)** 的失效率约等于大小为 **N/2** 的**两路组相联 Cache** 的失效率。
    -   *示例*: 表中 32KB 的1路 Cache 失效率 (0.020) 与 16KB 的2路 Cache 失效率 (0.022) 非常接近。

#### 三、 如何针对性地减少三类失效

根据 “3C” 模型的分析，可以针对性地采取措施来减少不同类型的失效：

1.  **要减少冲突失效**:
    -   **方法**: **提高相联度**。采用全相联就不会产生冲突失效。
    -   **代价**: 用硬件实现全相联是**很昂贵**的，并且可能会因为增加了比较和选择的逻辑而**降低处理器的时钟频率**，从而导致整体性能的下降。

2.  **要减少容量失效**:
    -   **方法**: **增大 Cache 的容量**。
    -   **代价**: 增加 Cache 容量会增加成本、功耗和潜在的命中时间。

3.  **要减少强制性失效**:
    -   **方法**: **增加块的大小 (Block Size)**。一次性调入更多的数据，可以减少首次访问的总次数。
    -   **代价**: 块大小增加可能会增加其他类型的失效（例如，如果块太大，有用的数据比例下降，可能会增加冲突和容量失效），并增加失效开销。

#### 四、 核心权衡

-   **重要提醒**: **许多降低失效率的方法会增加命中时间或失效开销**。
-   **最终目标**: 降低失效率的最终目的是为了**缩短总的 CPU 执行时间**或**平均访存时间 (AMAT)**。
-   **设计决策**: 在具体使用这些方法时，要**综合考虑**其对命中时间、失效率和失效开销的全面影响，保证所采取的措施确实能使整个系统速度提高。





### **5.4 降低Cache失效率的方法 (续)**

本节详细介绍了四种降低 Cache 失效率的具体方法，并分析了它们各自的优缺点和适用场景。

#### ■ 措施1：增加 Cache 块容量 (Block Size)

这是降低强制性失效最直接的方法。

##### 1. 失效率与块大小的关系
-   **U型曲线**: 对于给定的 Cache 容量，失效率随块大小变化的曲线通常呈 “U” 型。
    -   **开始下降**: 当块大小从较小值开始增加时（如从16B增加），失效率会**下降**。
    -   **后来上升**: 当块大小增加到一定程度后，失效率反而会**上升**。
-   **最佳点**: Cache 容量越大，使失效率达到最低的“最佳”块大小就越大。

##### 2. 增加块容量的双重作用
-   **正面作用 (减少强制性失效)**: 增大了块，利用了**空间局部性**。一次失效可以调入更多可能被访问的数据，从而减少了首次访问的总次数。
-   **负面作用 (可能增加冲突/容量失效)**: 在总容量不变的情况下，增大块大小意味着 **Cache 中块的数目减少**。
    -   这会导致多个主存块竞争少数 Cache 块位置的可能性增加，从而可能**增加冲突失效**。
    -   在 Cache 容量较小时，甚至会因为无法同时容纳程序所需的不同数据块而**增加容量失效**。

##### 3. 对失效开销和 AMAT 的影响
-   **增加失效开销**: 增加块大小**同时也会增加失效开销**，因为每次失效需要从下一级存储器传输更多的数据。
-   **权衡**: 如果增加块大小带来的**失效开销的负面效应**超过了**失效率下降带来的好处**，最终的**平均访存时间 (AMAT) 反而会增加**。
-   **结论**: 即使降低了失效率，也可能是得不偿失的。必须通过计算AMAT来找到最优的块大小。

##### 4. 块大小的选择策略
-   Cache 设计者一直在努力**同时减少失效率和失效开销**。
-   块大小的选择取决于下一级存储器的**延迟 (Latency)** 和**带宽 (Bandwidth)**。
    -   **高延迟和高带宽**时: 宜采用**较大的 Cache 块**。因为此时启动一次传输的代价很高，不如一次多传输一些数据。
    -   **低延迟和低带宽**时: 宜采用**较小的 Cache 块**。此时块的数量更多，有可能减少冲突失效。

---

#### ■ 措施2：提高相联度 (Associativity)

主要用于降低冲突失效。

-   **经验规则**:
    1.  **8路组相联的效果极限**: 在降低失效率方面，**8路组相联**的作用已经**基本和全相联一样有效**。采用超过8路的相联度实际意义不大，但成本和复杂性会急剧增加。
    2.  **2:1 Cache经验规则**: 容量为 **N** 的**直接映射 (1路)** Cache 的失效率，与容量为 **N/2** 的**两路组相联** Cache 的失效率差不多相同。

-   **代价**: 提高相联度是以**增加命中时间**为代价的。更高的相联度需要更复杂的比较逻辑（更多的比较器和更大的多路选择器），这会增加硬件延迟，可能导致处理器时钟频率降低。

---

#### ■ 措施3：增大 Cache 容量 (Cache Size)

主要用于降低容量失效。

-   **老的经验法则**: Cache 容量 (size) **增大一倍** (2x)，大约能**减小 25% 的缺失率**。
-   **作用机理**: 增大 Cache 容量主要减少了**容量缺失**和**冲突缺失**。
-   **代价**: 更大的 Cache 意味着更高的成本、功耗和潜在的更长的命中时间。

---

#### ■ 措施4：编译器优化 (Compiler Optimizations)

这是一种无需修改硬件，通过软件手段来降低失效率的方法。

-   **核心思想**: 利用编译器对**指令程序**和**数据**进行重排序，使其内存访问模式更加“Cache友好”，从而减小缺失率。

-   **指令优化**:
    -   例如，编译器预测某个转移会发生，可以将**转移目标基本块**与**转移指令后的基本块**进行代码位置互换，以提高空间局部性。

-   **数据优化**:
    -   **合并数组**: 将两个在同一循环中以相同方式访问的独立数组，合并为一个由复合元素组成的数组，从而改善空间和时间局部性。
    -   **循环交换 (Loop Interchange)**: 改变嵌套循环的顺序，以便按照数据在内存中的存储顺序（通常是按行）来访问数据。
    -   **循环融合 (Loop Fusion)**: 将有相同循环和一些变量重叠的两个独立循环组合成一个循环。

---

#### **总结与权衡**

-   **没有免费的午餐**: 改进平均访存时间 (AMAT) 的某一方面，通常是以损失另一方面为代价的。
    -   **增加块大小**在降低失效率的同时**增加失效开销**。
    -   **提高相联度**在降低失效率的同时**增加命中时间**。
-   **设计中的矛盾**:
    -   为了实现很高的处理器时钟频率，需要设计结构简单的 Cache（低相联度、小容量）。
    -   但时钟频率越高，失效开销就越大（以时钟周期数衡量），为了减少失效开销，又要求提高相联度和容量。
-   因此，Cache 设计是一个在**命中时间**、**失效率**、**失效开销**、**硬件复杂性**和**成本**之间进行精妙权衡的艺术。



## **5.5 减小Cache失效开销**

本节介绍了几种旨在缩短 Cache 失效后，从下一级存储获取数据所需时间的关键技术。

#### ■ 措施1：采用两级（多级）Cache

这是减少失效开销最基本、最有效的方法。

##### 1. 基本思想与公式
-   **思想**: 在原有的 Cache (L1) 和存储器之间增加另一级 Cache (L2)。
    -   **L1 Cache**: 容量相对较小，使其速度和快速 CPU 的时钟周期相匹配，主要目标是**缩短命中时间**。
    -   **L2 Cache**: 容量相对较大，能捕获更多本来需要到主存去的访问，主要目标是**降低 L1 的实际失效开销**。
-   **性能分析公式**:
    -   `平均访存时间 = 命中时间_L1 + 失效率_L1 × 失效开销_L1`
    -   `失效开销_L1 = 命中时间_L2 + 失效率_L2 × 失效开销_L2`
    -   **合并后**: `AMAT = HitTime_L1 + MissRate_L1 × (HitTime_L2 + MissRate_L2 × MissPenalty_L2)`

##### 2. 局部失效率 vs 全局失效率
-   **局部失效率 (Local Miss Rate)**: `该级Cache的失效次数 / 到达该级Cache的访存次数`。它衡量的是**本级 Cache**自身的表现。
-   **全局失效率 (Global Miss Rate)**: `该级Cache的失效次数 / CPU发出的访存总次数`。它衡量的是该级 Cache 对**整个系统**性能的最终影响。
-   **示例**:
    -   1000次访存，L1失效40次，L2失效20次。
    -   L1 失效率 (全局和局部) = `40/1000 = 4%`
    -   L2 局部失效率 = `20/40 = 50%`  (因为只有40次访问到达了L2)
    -   L2 全局失效率 = `20/1000 = 2%`

##### 3. L2 Cache 的设计考量
-   **核心区别**: L1 的速度会影响 CPU 的**时钟频率**和**命中时间**，而 L2 的速度只影响 **L1 的失效开销**。
-   **更大的设计空间**: 许多不适合 L1 的方案（如高相联度、大块）对于 L2 却可以使用。
-   **设计权衡**:
    1.  **容量**: L2 容量必须比 L1 **大许多**，否则 L2 的局部失效率会很高。通常 L2 容量很大（如8MB），使其基本没有容量失效，只剩下强制性失效和冲突失效。
    2.  **相联度**: 更高的相联度对 L2 降低失效率的作用更明显，但也会**增加命中时间**。设计时需要在降低失效率和增加命中时间之间权衡，选择能使 **L1 总失效开销最小**的方案。
    3.  **块大小**: 对于大容量的 L2 来说，增加块大小对冲突失效影响不大，因此经常使用 **64B、128B、甚至256B** 的大块来充分利用空间局部性。



#### ■ 措施2：关键字优先和提前重启

这两种技术旨在让 CPU 不必等待整个块传输完毕就能继续执行，从而减少停顿。

-   **方法**: CPU 只需要块中的一个字 (word) 就能继续工作。
-   **关键字优先 (Critical Word First / Requested Word First)**:
    -   首先从存储器请求缺失的那个**字**，并尽快地送回 CPU。
    -   CPU 继续执行的**同时**，在后台填充块中的其余字。也称为 **wrapped fetch**。
-   **提前重启 (Early Restart)**:
    -   以正常顺序（从块的起始地址）取块。
    -   只要块中所请求的字一到达，就送到 CPU，让 CPU 继续执行。
-   **适用性**: 这两种方法在**块较大**的情况下特别常用。基于**空间局部性**，程序很可能需要下一个连续的字，因此提前重启也是有利的。



#### ■ 措施3：让读失效优于写

此技术主要用于解决写缓冲带来的数据相关性问题，确保 CPU 能尽快获得所需数据。

##### 1. 问题：写后读 (Read After Write) 数据相关
-   **场景**: 在一个采用**写直达**和**写缓冲**的系统中，一个 `SW` (Store) 指令后紧跟着一个 `LW` (Load) 指令，且它们访问的地址可能会导致冲突。
    -   `SW R3, 512(R0)`
    -   `LW R1, 1024(R0)`
    -   `LW R2, 512(R0)`
-   **冲突过程**:
    1.  `SW` 发生写失效，数据 `R3` 被放入**写缓冲**等待写入主存。
    2.  第一个 `LW` 访问 `1024`，与 `512` 映射到同一 Cache 索引，导致读缺失，并从主存加载了地址 `1024` 所在的数据块，更新了 Cache。
    3.  第二个 `LW` 访问 `512`，再次发生读缺失。
-   **关键危险**: 如果此时写缓冲还未将 `R3` 的数据写入主存 `512` 地址，那么第二次读缺失就会从主存读回一个**错误的旧值**，导致 `R2` 的值不等于 `R3`。

##### 2. 解决方案
-   **方法一 (简单但低效)**: **推迟对读失效的处理，直到写缓冲清空**。
    -   **缺点**: 写缓冲中几乎总是有数据，这会大大增加读失效的开销。据估计，这会使读失效的平均开销增加 **50%**。
-   **方法二 (常用)**: **在读失效时检查写缓冲器的内容**。
    -   如果读地址与写缓冲中的地址**没有冲突**，则可以继续正常处理读失效。
    -   如果有**冲突**（地址匹配），则直接从写缓冲中获取数据，或者延迟读失效处理。

##### 3. 在写回 Cache 中的应用
-   类似的问题也存在于**写回 Cache**中。当读失效需要替换一个“脏”块时，常规操作是“先写脏块回主存，再读新块进 Cache”。
-   **优化**: 可以先把被替换的“脏”块拷入一个**写缓冲（或牺牲缓冲）**，然后立即开始读新块，最后再由缓冲慢慢写回主存。这样 CPU 的读访问就能更快完成。



#### ■ 措施4：Victim Cache (牺牲缓存)

-   **核心思想**: 一种能**减少冲突失效次数**而**不影响时钟频率**的方法。
-   **结构**: 在 L1 Cache 和它与下一级存储器（如 L2 Cache 或主存）的数据通路之间，增设一个**容量很小**（通常1~5项）、**全相联**的小 Cache，称为 Victim Cache。
-   **工作流程**:
    1.  Victim Cache 中存放由于**冲突失效而被替换出去**的那些块（即“牺牲品” victim）。
    2.  每当发生 L1 Cache 失效时，在访问下一级存储器**之前**，先检查 Victim Cache 中是否有所需的块。
    3.  **命中 Victim Cache**: 如果有，就将 Victim Cache 中的该块与 L1 Cache 中即将被替换的块进行**交换**。这次操作的开销远小于访问下一级存储器，从而显著减少了冲突失效的开销。
    4.  **未命中 Victim Cache**: 正常访问下一级存储器。
-   **效果**:
    -   Jouppi 于1990年发现，一个仅含 **1~5 项**的 Victim Cache 对减少冲突失效很有效，尤其是对于小型的直接映射数据 Cache。
    -   一个项数为 **4** 的 Victim Cache 能使一个 4KB 直接映射数据 Cache 的冲突失效**减少 20% ~ 90%**。

---

#### ■ 措施5：非阻塞 Cache (Non-blocking Cache)

-   **核心思想**: 允许 Cache 在处理一次**失效 (Miss)** 的过程中，继续响应 CPU 其他的**命中 (Hit)** 访问。
-   **别名**: 非锁定 Cache (Non-locking Cache)。
-   **优化措施**: 这种 **“失效下命中” (Hit Under Miss)** 的优化措施，在 Cache 失效时，不是完全拒绝 CPU 的访问，而是能处理部分命中访问，从而**减少了 CPU 的实际停顿时间**，即减小了实际的失效开销。
-   **进一步优化**: 如果允许**多个失效重叠 (Miss Under Miss)**，即同时处理多个失效请求，可以进一步减少失效开销。
-   **代价**:
    -   “失效下命中”措施**大大增加了 Cache 控制器的复杂度**，因为它这时可能需要有多个访存同时进行，需要复杂的逻辑来跟踪和处理这些并行的请求。
-   **效果**:
    -   从图表中可以看出，相比阻塞 Cache，支持“1次失效下命中”能将平均等待时间降低到原来的 **30%~80%**。支持“2次失效下命中”能进一步降低，但收益递减。

---

#### ■ <blockquote>措施6：预取技术 (Prefetching)</blockquote>

-   **核心思想**: 在处理器**正式发出访问请求之前**，就**提前**将指令和数据取到靠近处理器的位置。
-   **存放位置**: 预取的内容可以直接放入 Cache，也可以放在一个访问速度比主存快的**外部缓冲器**中。

##### 1. 指令预取 (Instruction Prefetch)
-   **实现**: 通常由 Cache 之外的**硬件**完成。
-   **示例 (Alpha AXP 21064)**:
    -   发生指令 Cache 失效时，硬件会同时取**两个块**：**被请求的指令块**和**顺序的下一个指令块**。
    -   被请求的指令块返回时放入 **Cache**。
    -   被预取的下一个指令块则放入一个专门的**指令缓冲器**中。
    -   如果某次被请求的指令正好在缓冲器里，则取消对该块的访存请求，直接从缓冲器中读出，并同时发出对下一个指令块的预取请求。

##### 2. 数据预取 (Data Prefetch)
-   **硬件数据预取**: 可以用类似的技术预取数据。统计表明，一个数据流缓冲器大约可以捕获一个 4KB 直接映射 Cache 的 **25%** 的失效。
-   **软件预取**:
    -   **思想**: 硬件预取的一种替代方法是在**编译时**由编译器加入**预取指令**，在数据被用到之前发出预取请求。
    -   **两种类型**:
        1.  **寄存器预取 (Register Prefetch)**: 把数据直接预取到**寄存器**中。
        2.  **Cache 预取 (Cache Prefetch)**: 只将数据预取到 **Cache** 中，不放入寄存器。

-   **效果**:
    -   对于一个 4KB 直接映射指令 Cache，一个16块的指令缓冲器可以捕获约 **72%** 的失效。



## **5.6 减小Cache命中时间**

继讨论减少失效次数和失效开销之后，本节聚焦于优化平均访存时间 (AMAT) 的第三个组成部分——**命中时间 (Hit Time)**。

#### 一、 减小命中时间的重要性

-   **直接影响 AMAT**: 命中时间是 AMAT 公式 `(AMAT = Hit Time + Miss Rate × Miss Penalty)` 的基础组成部分，直接决定了最常见情况下的访存速度。
-   **制约处理器时钟频率**: 更为关键的是，**命中时间会影响到处理器的时钟频率**。
    -   在实际机器中，往往是 **L1 Cache 的访问时间限制了整个处理器系统的时钟频率**。因为 Cache 访问是处理器执行指令的关键路径之一，其耗时决定了时钟周期的下限。
-   **结论**: 减少命中时间不仅对于减小 AMAT 很重要，而且对于提升处理器整体时钟频率和其他许多方面也是非常重要的。

---

#### ■ 措施1：采用容量小、结构简单的 Cache

-   **命中过程中的耗时部分**: 用地址的索引部分访问 Cache，读出标记并与地址进行比较，是 Cache 命中过程中最耗时的部分。
-   **核心思想**: 为了得到高速的系统时钟频率，**第一级 Cache (L1 Cache)** 应当选用容量小且结构简单的设计方案。
-   **具体建议**:
    1.  **小容量 Cache**:
        -   应使 Cache 容量足够小，以便可以与处理器做在**同一芯片 (on-chip)** 上，避免因芯片外访问而增加时间开销。
        -   物理上更小的电路通常延迟更低。
    2.  **简单 Cache 结构**:
        -   例如，采用**直接映射 (Direct Mapped)** Cache。
        -   直接映射 Cache 的主要优点是，**地址索引**和**标记检测**可以**并行进行**，而数据的传送也可以与标记检测**重叠进行**，这可以有效地减少总的命中时间。

---

#### ■ 措施2：写操作流水化 (Pipelined Writes)

-   **问题**: 写命中通常比读命中花费更多的时间，因为在写入数据之前必须先检测 Tag 以确认命中。
-   **核心思想**: 可以通过把**写操作流水化**来提高写命中的速度，使得从 CPU 的角度看，每个时钟周期都能完成一个写操作。
-   **实现方法 (以 Alpha AXP 21064 为例)**:
    -   Tag 和数据是分开存放的，因而能分别独立访问。
    -   每个写操作被分为**两个阶段**来完成：
        1.  **第一阶段 (Tag 比较)**: 进行 Tag 比较，并将 Tag 和数据暂存入延迟写缓冲器中。
        2.  **第二阶段 (数据写入)**: 若命中，则在下一个阶段再进行数据的写入。
    -   **流水线工作**: 这两个阶段按流水方式工作。当前“写”的 **Tag 比较**阶段，可以和上一个“写”的**数据写入**阶段**并行起来**。
-   **效果**: 通过流水化，虽然一个完整的写操作需要两个阶段，但由于重叠执行，可以实现每个时钟周期完成一个写操作的吞吐率，从而提高了写操作的有效速度。



## **5.7 虚拟存储器**

#### 一、 虚拟存储器的基本概念与目的

1.  **定义**:
    -   虚拟存储器（又称虚拟内存），是一种将**主存 (DRAM)** 用作**辅助存储器 (磁盘)** 的**高速缓存**的技术。
    -   通俗地说，就是将**磁盘的一部分当作内存来使用**。

2.  **目的**:
    -   **更有效地共享处理器和主存**: 多个程序可以同时存在于内存中，提高了系统资源的利用率。
    -   **可以运行超过主存容量的程序**: 程序的大小不再受物理内存大小的限制。

3.  **工作流程**:
    -   基于冯·诺依曼的“存储程序概念”，要运行的程序必须先从磁盘**调入**内存，然后CPU才能运行。
    -   在虚拟存储器系统中，处理器产生的是一个**虚拟地址 (Virtual Address)**，这个地址需要先**转换**成一个**物理地址 (Physical Address)**，然后才能访问主存。

#### 二、 地址空间：虚拟地址 vs 物理地址

1.  **虚拟地址 (Virtual Address)**:
    -   用户编程时使用的地址空间，也称为**逻辑地址**。
    -   每个程序（如程序A、程序B）都拥有自己**独立、连续、从0开始**的虚拟地址空间，它们之间互不干扰。

2.  **物理地址 (Physical Address)**:
    -   物理内存的真实地址空间。
    -   是有限的、硬件决定的实际存储位置。

#### 三、 虚拟存储器与 Cache 的类比

-   **工作原理**: 虚拟存储器和 Cache 的工作原理是**一样的**，都是利用高速存储（主存/Cache）来缓存低速存储（磁盘/主存）中的部分数据。
-   **不同术语**: 由于历史原因，它们使用不同的术语：
| 概念             | Cache         | 虚拟存储器            |
| :--------------- | :------------ | :-------------------- |
| **基本管理单位** | 块 (Block)    | **页 (Page)**         |
| **未命中**       | 块缺失 (Miss) | **缺页 (Page Fault)** |

-   **额外功能**: 虚拟存储器还提供**重定位 (Relocation)** 功能，以简化运行时程序的加载过程。重定位允许我们将程序加载到主存中的**任何位置**，因为虚拟地址到物理地址的映射是动态的。

#### 四、 虚拟地址到物理地址的转换

地址转换是虚拟存储器系统的核心机制。

1.  **地址结构**:
    -   **虚拟地址**: `虚拟页号 (Virtual Page Number) | 页内偏移 (Page Offset)`
    -   **物理地址**: `物理页号 (Physical Page Number) | 页内偏移 (Page Offset)`
    -   **关键**: **页内偏移**在虚拟地址和物理地址中是**相同**的，地址转换的本质就是将**虚拟页号**翻译成**物理页号**。

2.  **映射过程**:
    -   通过**页表 (Page Table)** 进行虚拟存储器-物理存储器映射。页表存储了虚拟页号到物理页号的对应关系。
    -   **地址命中 (在内存中)**:
        -   CPU发出虚拟地址。
        -   通过页表查找，发现对应的页在内存中。
        -   完成地址转换，进行存取。
    -   **缺页 (Page Fault - 在磁盘中)**:
        1.  页表查找失败，硬件产生**缺页异常**，将控制权交给操作系统(OS)。
        2.  当前程序被**挂起**。
        3.  **OS** 负责将磁盘中的页**换入**内存（如果内存已满，还需执行替换算法）。
        4.  更新页表。
        5.  换页完成后，**重启**被挂起的程序（从导致缺页的那条指令重新执行）。

#### 五、 虚拟存储器设计时需要考虑的问题

虚拟存储器的设计与 Cache 的设计非常相似，也需要回答四个关键问题：

1.  **页的大小设置**:
    -   典型值为 **4KB ~ 16KB**。页的大小远大于 Cache 块，因为磁盘访问的延迟极高，一次多传输一些数据更划算。
2.  **降低缺页率 (映射规则)**:
    -   由于缺页开销巨大，必须尽可能降低缺页率。因此，通常采用**组相联或全相联**的映射方式。
3.  **缺页的替换算法**:
    -   当内存已满时，需要选择一个页进行替换。常用的有 LRU (最近最少使用) 或其近似算法。
4.  **写策略**:
    -   由于**写磁盘太长**，所以**必须采用写回 (Write-Back) 机制**，而不能使用写直达 (Write-Through)。



#### 六、 虚拟存储器的管理方式：页式 vs 段式

1.  **页式虚拟存储 (Paging)**:
    -   **方式**: 使用**固定大小的块**，即**页 (Page)**。
    -   **特点**: **分页使页号和偏移量的界限对于程序员和编译器不可见**。地址转换过程由硬件和操作系统自动完成，对上层透明。
    -   **现状**: 这是本课程讨论的重点，也是现代操作系统最主流的方式。

2.  **段式虚拟存储 (Segmentation)**:
    -   **方式**: 使用**可变长度的块**，即**段 (Segment)**。
    -   **地址构成**: `段号 + 段内偏移`。
    -   **地址转换**: `段寄存器 (存放段基址) + 段内偏移` 得到实际物理地址。
    -   **缺点**: 需要**程序员管理**分段，增加了编程复杂性。

#### 七、 页的存放和查找：页表 (Page Table)

-   **映射方式**: 虚拟存储系统通常采用**全相联映射**，即一个虚拟页可以映射到**任何一个**物理页。这使得操作系统 (OS) 在缺页发生时，可以选择任意一个物理页进行替换，灵活性最大。
-   **定位问题**: 全相联映射的困难在于定位。全部进行检索是不现实的。
-   **解决方案**: 使用**页表 (Page Table)** 来定位页。
    -   **存放位置**: 页表存放在**主存储器**中。
    -   **归属**: **每个进程 (Process) 都有自己独立的页表**。
    -   **寻址**: 硬件用一个指向页表首地址的专用寄存器，即**页表基址寄存器 (Page Table Base Register)**，来指出当前进程的页表在存储器中的位置。

-   **页表项 (Page Table Entry - PTE) 结构**:
    -   页表中的每一项对应一个虚拟页。
    -   **有效位 (Valid Bit)**: `1` 表示该页在主存中，`0` 表示不在。
    -   **物理页号 (Physical Page Number)**: 如果有效位为1，这里存放对应的物理页号。
    -   **磁盘地址 (Disk Address)**: 如果有效位为0，这里存放该页在磁盘上的位置。

#### 八、 缺页处理与页的调入/置换

1.  **缺页 (Page Fault)**:
    -   如果访问一个虚拟地址时，其对应的页表项中**有效位是0**，就发生**缺页**。
    -   控制权通过**异常机制**转移给**操作系统 (OS)**。
    -   OS 必须在下一级存储层次（磁盘）中找到该页，然后决定将该页放到主存的什么位置，并更新页表。

2.  **缺页页面的调进方法**:
    -   **预调方式 (Prefetching)**: 预测将要用到的页面，提前调入内存。
    -   **请求方式 (Demand Paging)**: 根据请求将需要的页机动调入内存。
    -   **现状**: 由于预测哪些页面将要用到是比较困难的，故**多采用请求方式**。

3.  **页面的置换方法**:
    -   **基本策略**: **最近最少使用策略 (LRU - Least Recently Used)**。
    -   **问题**: 要**完全准确地实现LRU算法代价较大**。
    -   **近似方法**: 采用**引用位 (Reference Bit / Use Bit)**。
        -   当一页被访问时，硬件自动将其引用位置位 (`1`)。
        -   OS 定期将所有页的引用位清零。
        -   这样，OS 就可以判断在这段特定时间内哪些页被访问过，从而可以从那些**最近没有访问过**的页（引用位仍为`0`）中选择一页进行替换。

#### 九、 页表容量问题

-   **问题**: 页表本身可能会非常大，占用大量内存空间。
-   **计算示例**:
    -   **假设**: 32位虚拟地址，页大小4KB，页表每项4字节。
    -   `页表项数 = 2^32 / 2^12 = 2^20` (约一百万项)
    -   `页表容量 = 2^20 × 4B = 4 MB`
-   **影响**: 每个程序执行时都需要 `4MB` 的存储空间来存放自己的页表。如果成百上千个程序同时执行，这将是巨大的开销。
-   **思路**: 采用**多级页表 (Multi-level Page Table)** 等技术来解决这个问题。

#### 十、 关于写操作

-   **写策略**: 虚拟存储系统**必须使用写回 (Write-Back) 机制**。
    -   **原因**: 写磁盘的操作非常耗时，写直达 (Write-Through) 的性能开销无法接受。把整页复制回磁盘比把单个字写回要高效很多。
-   **脏页跟踪**:
    -   当某一页被替换时，我们需要知道该页是否需要被写回（即内容是否被修改过）。
    -   为了追踪页是否被写过，可在页表中增加一个**重写位 (Dirty Bit)**。
    -   当页中任何字被写时，硬件自动将该页的 `Dirty Bit` 置位。替换时，OS 只需写回 `Dirty Bit` 为 `1` 的页。
    -   

## **5.8 加快地址转换：TLB **

#### 一、 为什么需要 TLB？

1.  **性能瓶颈**: 由于**页表存放在主存中**，因此程序每次访存，为了完成地址转换，实际上**最少需要2次内存访问**：
    -   **第1次访存**: 访问页表，查找到虚拟页对应的物理页号，从而**取得物理地址**。
    -   **第2次访存**: 使用得到的物理地址，**真正地取得数据**。
    -   这使得内存访问的性能下降了一半，是无法接受的。

2.  **解决方案：利用页表访问的局部性**
    -   程序在一段时间内很可能只访问少数几个页，因此对页表的访问也表现出强烈的**局部性**。
    -   可以设置一个特殊的 **Cache**，用于追踪最近使用过的地址变换。这个放置页表一部分的 Cache 就称为**快表 (Translation Lookaside Buffer, TLB)**。

#### 二、 TLB 的工作原理与特性

1.  **TLB是什么**: TLB 本质上是一个**页表项的高速缓存**，通常采用**全相联**或**高路组相联**的方式构建，以降低冲突失效。
2.  **典型值**:
    -   **大小**: 16 ~ 512 个项 (Entry)。
    -   **块大小**: 每个项对应 1 ~ 2 个页表项。
    -   **命中时间**: 0.5 ~ 1 个时钟周期。
    -   **缺失代价**: 10 ~ 100 个时钟周期。
    -   **缺失率**: 0.01% ~ 1% (非常低)。
    -   **缺失处理**: 采用硬件或软件处理。
    -   **替换策略**: 多采**用随机策略**或伪LRU。

#### 三、 TLB 缺失 (TLB Miss)

当 TLB 中没有一个表项能匹配虚拟地址时，TLB 缺失就会发生。

1.  **两种可能**:
    -   **(1) 页在主存中**: 对应的页表项在主存的页表中，只是不在 TLB 里。
    -   **(2) 页不在主存中 (缺页)**: 对应的页表项也在磁盘上。
2.  **如何判断**:
    -   处理 TLB 缺失时，需要查找主存中的**页表**。
    -   如果查找到的页表项的**有效位是1**，则属于情况(1)。
    -   如果查找到的页表项的**有效位是0**，则属于情况(2)，即发生了**缺页**。
3.  **TLB 缺失处理流程**:
    -   **MIPS (软件实现)**:
        -   **情况(1)**: 硬件捕获异常，由**异常处理程序**（软件）从主存中取出对应的页表项，装入 TLB。然后重新执行引起 TLB 缺失的那条指令，此时 TLB 命中。
        -   **情况(2) - 缺页**: 将控制权交给**操作系统 (OS)**。
            1.  OS 使用虚拟地址查找页表项，并在磁盘行找到被访问的页。
            2.  选择替换一个物理页。
            3.  启动读磁盘操作，将被访问的页从磁盘取回到物理页上。
            4.  当读磁盘操作完成后，OS 恢复原先引起缺页的进程状态，并执行从异常返回的指令，程序重新执行。
4.  **数据访问缺页的复杂性**:
    -   数据访问引起的缺页异常通常很难处理，因为：
        -   它们发生在指令执行的**中间**，不同于指令缺页。
        -   在异常处理前指令没有结束。
        -   异常处理之后，指令**必须重新执行**，就好像什么都没有发生一样。

#### 四、 TLB 与 Cache 的关系

TLB 和 Cache 是存储层次结构中紧密协作的两个组件，共同完成从虚拟地址到最终数据的获取。

1.  **基本流程**:
    -   CPU 发出**虚拟地址 (Virtual Address)**。
    -   地址首先被送到 **TLB** 进行查询。
    -   **TLB 命中**: 快速得到**物理地址 (Physical Address)**。
    -   **TLB 缺失**: 访问主存中的页表（可能进一步导致缺页），找到物理地址后更新 TLB。
    -   得到的**物理地址**被送到 **Cache** 进行查询。
    -   **Cache 命中**: 从 Cache 中获得数据。
    -   **Cache 缺失**: 从主存中获得数据。
2.  **最好情况**:
    -   虚拟地址由 **TLB 转换** (命中)，然后送至 **Cache** 找到相应的数据 (命中)，然后取回并送入处理器。
3.  **最坏情况**:
    -   **TLB、页表、Cache** 三个部件都发生缺失 (TLB Miss -> Page Fault -> Cache Miss)。

#### 五、 TLB、页表、Cache 的命中/缺失组合分析

| TLB  | 页表 | Cache | 可能发生吗？                                                 |
| :--- | :--- | :---- | :----------------------------------------------------------- |
| 命中 | 命中 | 缺失  | **可能**，但若TLB命中就不会再查页表。                        |
| 缺失 | 命中 | 命中  | **可能**，TLB缺失后查页表找到，用物理地址在Cache中找到数据。 |
| 缺失 | 命中 | 缺失  | **可能**，TLB缺失后查页表找到，但用物理地址在Cache中没找到数据。 |
| 缺失 | 缺失 | 缺失  | **可能**，TLB缺失，进而缺页，从磁盘加载后在Cache中也没找到。 |
| 命中 | 缺失 | 缺失  | **不可能**，若页表缺失（缺页），该转换不可能在TLB中。        |
| 命中 | 缺失 | 命中  | **不可能**，同上。                                           |
| 缺失 | 缺失 | 命中  | **不可能**，若页不在主存中，其数据也不可能出现在Cache中。    |

-   **并行操作的可能性**: 为了进一步加速，可以利用地址中**页内偏移**和 **Cache 索引**的部分位重叠的特性，实现 **TLB 查询和 Cache 索引查询并行**进行，这种技术称为**虚拟索引物理标记 (VIPT)** Cache。



## **5.9 虚拟存储器的保护**

除了提供更大的地址空间，虚拟存储器还有一个至关重要的功能：**保护 (Protection)**。

#### 一、 保护功能的目标

虚拟存储器的保护机制旨在实现以下目标：
-   允许多个进程 (Process) **安全地共享一个单一的主存**。
-   为每个进程提供**独立的运行空间**。
-   **保护每个进程的空间不被另外的进程非法访问**，即防止一个进程读取或写入另一个进程的数据。

#### 二、 基本原理：两种操作模式

为了实现保护，处理器提供了至少两种操作模式（特权级别）：

1.  **用户模式 (User Mode / 用户态)**:
    -   这是普通应用程序运行的模式。
    -   在此模式下，程序**只能访问用户自己的虚拟地址空间**。
    -   任何试图访问操作系统空间、修改页表或执行特权指令的操作都将被硬件禁止并引发异常。

2.  **超级用户管理模式 (Supervisor Mode / 核心态或管态)**:
    -   这是操作系统 (OS) 内核运行的模式，拥有最高权限。
    -   在此模式下，程序**允许访问 OS 的地址空间、页表**以及其他核心系统状态信息，并可以执行所有特权指令。

#### 三、 模式切换：系统调用 (System Call)

程序需要一种可控的方式从用户模式切换到管理模式，以请求操作系统提供的服务（如文件I/O），这就是系统调用。

-   **定义**: 将控制权从**用户模式**转换到**管理员模式**的**特殊指令**（如 MIPS 指令集中的 `syscall`）。
-   **工作流程**:
    1.  用户程序执行 `syscall` 指令。
    2.  硬件触发一个**异常 (Exception)**。
    3.  与处理其他异常一样，当前指令的地址（程序计数器PC的值）被保存在**异常程序计数器 (EPC)** 中。
    4.  处理器被自动置于**管理态**。
    5.  控制权转移到操作系统预设的异常处理程序（系统调用入口）。
    6.  操作系统完成服务后，执行从异常返回的指令，将处理器恢复到用户态，并从 EPC 中保存的地址继续执行用户程序。

#### 四、 实现进程隔离与共享

1.  **进程隔离**:
    -   **核心机制**: 每个进程都有**自己独立的虚拟地址空间和页表**。
    -   **操作系统保障**: 操作系统负责管理页表的组织，确保**独立的虚拟页映射到不相交的物理页上**。这样，一个进程就无法通过其虚拟地址访问到另一个进程的物理数据。
    -   **页表保护**: 页表本身存放在**操作系统的保护地址空间**中，防止用户进程随意更改自己的页表，从而绕过保护机制。

2.  **进程共享**:
    -   虽然进程空间默认是隔离的，但虚拟存储器也允许多个进程**共享**它们虚拟地址空间中的一部分。
    -   这是通过操作系统将不同进程的不同虚拟页，**映射到同一个物理页**来实现的。这对于共享库（如动态链接库DLL）和进程间通信非常重要。

#### 五、 总结：硬件如何支持保护

硬件通过以下机制来支持操作系统对虚拟地址空间的保护：
-   提供**指示处理器被置于超级用户管理模式**（核心态或管态）的机制。
-   通过**系统调用异常处理**来完成受控的模式切换。
-   提供**特殊指令**（如 `syscall`）将控制权安全地传送到管理代码空间的指令位置。



### **虚拟机 (Virtual Machine, VM)**

虚拟机是一种强大的抽象技术，它允许在一台物理计算机上模拟出多个独立的、完整的计算机环境。

#### 一、 虚拟机概念与发展

1.  **历史**: 从20世纪60年代到现在，虚拟机一直是**大型机**的重要组成部分。
2.  **近年来受到关注的原因**:
    -   **隔离性和安全性的重要性日益增长**。
    -   标准操作系统在安全性和可靠性方面存在缺陷。
    -   多个不相关的用户间需要**共享计算机**资源。
    -   **硬件能力大幅增加**，使得运行虚拟机的额外开销降低至可接受范围内。

#### 二、 虚拟机的工作模型
-   **核心**: 虚拟化平台（如 VMware）在底层硬件（如 x86 Architecture）之上构建一个**虚拟化层 (Virtualization Layer)**。
-   **功能**: 这个虚拟化层为上层的多个**客户操作系统 (Guest OS)** 虚拟出相应的硬件环境（CPU, Memory, NIC, Disk）。
-   **效果**: 每个客户操作系统都认为自己独占了一台完整的计算机，可以在其上运行自己的应用程序 (App)。这些操作系统可以是相同或不同的。

#### 三、 虚拟机分类
1.  **应用级虚拟机**:
    -   **例子**: Java虚拟机 (JVM)。
    -   **特点**: 在一个操作系统之上，为特定的应用程序创建一个跨平台的、统一的运行环境。
2.  **系统级虚拟机**:
    -   **例子**: Xen, IBM VM/370。
    -   **特点**: 让用户觉得**自己独享计算机**。一台运行多个虚拟机的物理计算机可以支持多个不同的OS，这些OS共享底层的硬件资源。

#### 四、 虚拟机管理器 (VMM / Hypervisor)

-   **定义**: 管理虚拟机的软件被称为**虚拟机管理器 (Virtual Machine Monitor, VMM)**，或**管理程序 (Hypervisor)**。
-   **VMM 必备条件**:
    1.  使客户软件在虚拟机中的运行和在本地硬件上的运行**效果完全相同**。
    2.  客户软件**不能直接改变**系统的资源分配。
    3.  VMM 能在必要时**控制一切**，包括访问特权状态、地址转换、I/O、异常和中断等。
    4.  VMM 的特权级比客户虚拟机的特权级**更高**。
-   **VMM 的工作机制**:
    -   VMM 必须保证客户系统只能和**虚拟资源**交互。
    -   如果客户OS试图通过**特权指令**访问或修改相关硬件资源（如写一个页表指针），这个操作会被 VMM **截获 (trap)** 并处理。
    -   **硬件支持**: 这要求处理器硬件能够支持：任何试图在**用户模式**下读/写敏感信息的指令，都能被 VMM 截获并处理。
    -   **挑战**: 不幸的是，大部分指令集（如早期的x86）在创建时**都没有考虑对虚拟化的支持**，这使得虚拟化的实现变得复杂。

---

### **补充：并行与存储器层次结构：Cache的一致性**

当多个处理器（核）共享一个物理地址空间时，如何保证各个处理器 Cache 中数据的副本与主存数据一致，就成为了一个关键问题，即 **Cache 一致性 (Cache Coherence)**。

#### 一、 一致性问题示例

| Time | Event                   | CPU A's cache | CPU B's cache | Memory |
| :--- | :---------------------- | :------------ | :------------ | :----- |
| 0    |                         |               |               | 0      |
| 1    | CPU A reads X           | 0             |               | 0      |
| 2    | CPU B reads X           | 0             | 0             | 0      |
| 3    | **CPU A writes 1 to X** | **1**         | **0**         | **1**  |

-   **问题**: 在第3步，CPU A 将 X 的值更新为1。在写直达策略下，CPU A 的 Cache 和主存的值都变成了1，但 CPU B 的 Cache 中 X 的值**仍然是旧的 0**。如果此时 CPU B 读取 X，就会得到一个过时的错误数据。

#### 二、 一致性 (Coherence) 的定义

1.  **通俗定义**: 读操作时总是返回**最近写**的值。
2.  **正式定义**:
    -   **保证程序执行顺序**: 如果处理器 P 写入 X，之后 P 再读取 X（中间无其他处理器写X），则 P 读出的必须是它自己写入的值。
    -   **定义了存储的一致性**: 如果处理器 P1 写入 X，其他处理器与 X 相应的 Cache 必须被**更新**（或失效）。
    -   **串行化写入**: 如果 P1 写入 X，P2 也写入 X，所有处理器看到的写入顺序必须是**相同的**。

#### 三、 最常用的 Cache 一致性协议

-   **监听 (Snooping) 协议**:
    -   **工作机制**:
        1.  每个 Cache 都**监听 (snoop)** 共享总线 (bus) 上的所有读/写操作。
        2.  每个 Cache 和主存都在一个目录中记录数据块的**共享状态**。
    -   **写操作处理**:
        -   当处理器要写数据时，它以**独占方式**对该数据块进行写。
        -   首先，在总线上广播一个 **“写无效 (Invalidate)”** 消息。
        -   其他所有 Cache 收到此消息后，会将自己本地的该数据块副本**标记为无效**。
        -   随后，当其他 Cache 需要读取该数据时，会发生一次读缺失，从而从主存或持有最新副本的 Cache 中获取更新后的数据。

-   **监听协议下的工作流程**:
| CPU activity            | Bus activity         | CPU A's cache | CPU B's cache | Memory |
| :---------------------- | :------------------- | :------------ | :------------ | :----- |
|                         |                      |               |               | 0      |
| CPU A reads X           | Cache miss for X     | 0             |               | 0      |
| CPU B reads X           | Cache miss for X     | 0             | 0             | 0      |
| **CPU A writes 1 to X** | **Invalidate for X** | **1**         | **Invalid**   | 1      |
| **CPU B reads X**       | **Cache miss for X** | 1             | **1**         | 1      |

-   **效果**: 在第5步，当 CPU B 再次读取 X 时，由于其本地副本已无效，会触发一次 Cache Miss，从而从主存获取到了 CPU A 写入的最新值 `1`，保证了数据的一致性。
